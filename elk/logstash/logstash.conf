input {
  file {
    path => "/usr/share/logstash/ingest_data/application.log"
    sincedb_path => "/usr/share/logstash/data/sincedb_application"
  }
}

filter {
    grok {
      match => {
        "message" =>
          "^\[%{TIMESTAMP_ISO8601:created_at}\]\s+SecurityIncidentReport saved: id=%{UUID:database_id},\s+employee=%{DATA:employee_full_name},\s+securityOrg=%{DATA:security_organization_name},\s+affectedOrg=%{DATA:affected_organization_name},\s+severity=%{WORD:incident_severity},\s+address=%{DATA:affected_organization_address},\s+city=%{DATA:affected_organization_city},\s+content=%{GREEDYDATA:content},\s+file=%{GREEDYDATA:file_path}\s*$"
      }
    }
      if "_grokparsefailure" in [tags] {
        drop { }
      }


    mutate{
        rename => { "message" => "text_field"}
        gsub => [
          "text_field", "\r", "",
          "[event][original]", "\r", ""
        ]
        add_field => { "location" => "%{lat},%{lon}" }
        remove_field => ["@timestamp", "@version", "host", "log", "event"]
    }
}

output{

    #data is sent to elasticsearch as output
    elasticsearch{
        hosts => ["http://elasticsearch:9200"]
        user => "ml_user"
        password => "NewStrongPassword123"
        index => "security_incident_report_index"
        #pipeline => "text-embedding-pipeline-all-MiniLM-L6-v2"
    }
    stdout{
        codec => rubydebug
    }
}