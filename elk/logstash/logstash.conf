input {
  file {
    path => "/usr/share/logstash/ingest_data/application.log"
    sincedb_path => "/usr/share/logstash/data/sincedb_application"
  }
}

filter {
    grok {
        match => {
        "message" => [
                "^\[%{TIMESTAMP_ISO8601:created_at}\]\s+IncidentReport saved: id=%{UUID:database_id},\s+employee=%{DATA:employee_full_name},\s+securityOrg=%{DATA:security_organization_name},\s+attackedOrg=%{DATA:attacked_organization_name},\s+severity=%{WORD:incident_severity},\s+attackedOrgAddress=%{GREEDYDATA:attacked_organization_address},\s+attackedOrgCity=%{GREEDYDATA:attacked_organization_city},\s+content=%{GREEDYDATA:content},\s+file=%{GREEDYDATA:file_path}\s*$"
              ]
    }
    if "_grokparsefailure" in [tags] {
        drop { }
    }

    mutate{
        rename => { "message" => "text_field"}
        gsub => [
          "text_field", "\r", "",
          "[event][original]", "\r", ""
        ]
        add_field => { "location" => "%{lat},%{lon}" }
        remove_field => ["@timestamp", "@version", "host", "log", "event"]
    }
}

output{

    #data is sent to elasticsearch as output
    elasticsearch{
        hosts => ["http://elasticsearch:9200"]
        user => "ml_user"
        password => "${ES_PASSWORD}"
        index => "incident_report_index"
        pipeline => "text-embedding-pipeline-all-MiniLM-L6-v2"
    }
    stdout{
        codec => rubydebug
    }
}